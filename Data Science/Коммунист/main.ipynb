{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eWaYdWePD3Z"
      },
      "source": [
        "# Коммунист\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "In this activity, you will recognise characters from the Cyrillic script.\n",
        "\n",
        "Raw data is presented as JPG images.\n",
        "You will have to submit your predictions in CSV though, fill up the empty label coloum with your predicted Cyrillic character in test.csv\n",
        "\n",
        "You may choose to use standard sklearn models or, if you want to challenge yourself, a simple CNN for a good score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8norIBGPRyj"
      },
      "source": [
        "## Solution\n",
        "\n",
        "We found the original Cyrillic characters dataset online and we will be training our model on that.\n",
        "\n",
        "https://github.com/GregVial/CoMNIST\n",
        "\n",
        "We will be using tensorflow and keras as backend for our model.\n",
        "\n",
        "**This code is meant for Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBhp02tzZSaK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL_WjFdtZlmn"
      },
      "outputs": [],
      "source": [
        "!tar -xf /content/drive/MyDrive/Cyberthon/CoMNIST.package.tar.xz\n",
        "!unzip -qq /content/drive/MyDrive/Cyberthon/Cyrillic.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp9NLPGNwkNa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import psutil\n",
        "import keras\n",
        "import math\n",
        "import datetime\n",
        "import platform\n",
        "import random\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Input\n",
        "from keras.activations import relu, softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyWxLxSFRxvh"
      },
      "source": [
        "## Data Loading & Importing\n",
        "\n",
        "The **training** dataset consists of `15480` images which we will resize to `100px` by `100px`. All images are grayscaled, meaning they do not have any color. Each pixel is a number between 0 and 255 representing how white or black it is.\n",
        "\n",
        "The images had some issues and I couldn't read them as grayscale, so I had to use cv2.IMREAD_UNCHANGED and remove the RGB channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyllQhiYaLOP"
      },
      "outputs": [],
      "source": [
        "image_folder = \"Cyrillic\"\n",
        "images, labels = [], []\n",
        "\n",
        "unique_labels = os.listdir(image_folder)\n",
        "convert = {char: idx for idx, char in enumerate(unique_labels)}\n",
        "alphabet = {idx: char for idx, char in enumerate(unique_labels)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for label in unique_labels:\n",
        "    for filename in os.listdir(os.path.join(image_folder, label)):\n",
        "        image_path = os.path.join(image_folder, label, filename)\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "        image = cv2.resize(image, (100, 100))\n",
        "\n",
        "        image = image[:, :, -1]\n",
        "        images.append(image)\n",
        "        labels.append(convert[label])"
      ],
      "metadata": {
        "id": "JYxRyO-w1FNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the lists to numpy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "Wzzr3pGo1GtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtvvnCCjSgJZ"
      },
      "outputs": [],
      "source": [
        "print('x_train:', x_train.shape)\n",
        "print('y_train:', y_train.shape)\n",
        "print('x_test:', x_test.shape)\n",
        "print('y_test:', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgA9E73ISnzU"
      },
      "outputs": [],
      "source": [
        "# Save image parameters to the constants that we will use later for data re-shaping and for model training.\n",
        "(_, IMAGE_WIDTH, IMAGE_HEIGHT) = x_train.shape\n",
        "IMAGE_CHANNELS = 1\n",
        "\n",
        "print('IMAGE_WIDTH:', IMAGE_WIDTH)\n",
        "print('IMAGE_HEIGHT:', IMAGE_HEIGHT)\n",
        "print('IMAGE_CHANNELS:', IMAGE_CHANNELS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jzm9JaoSsWu"
      },
      "source": [
        "## Exploratory Data Anaylsis\n",
        "\n",
        "It is important to explore our dataset as we will get to know how our data looks like. Then, we can preprocess and reshape our data accordingly. We can visualize our data using the various libraries we have imported."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgzHBNzATexH"
      },
      "source": [
        "Displaying a random image from our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HelqIIcSTUbi"
      },
      "outputs": [],
      "source": [
        "plt.imshow(random.choice(x_train), cmap=plt.cm.binary)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVCqYN0-UhMM"
      },
      "source": [
        "Let's print some more training examples to get the feeling of how the characters were written."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA_lmV9xUKe9"
      },
      "outputs": [],
      "source": [
        "amount_to_display = 25\n",
        "num_cells = math.ceil(math.sqrt(amount_to_display))\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(amount_to_display):\n",
        "    index = random.randint(0, x_train.shape[0])\n",
        "    plt.subplot(num_cells, num_cells, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x_train[index], cmap=plt.cm.binary)\n",
        "    plt.xlabel(alphabet[y_train[index]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC-VHE5oUnIx"
      },
      "source": [
        "Let's plot and visualize the distribution of data for each class ... They seem to be quite balanced, which is awesome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRwR7B_aUxf2"
      },
      "outputs": [],
      "source": [
        "sn.countplot(x=pd.DataFrame(y_train)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ROzmrpWW5Ge"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Now that we have explored our data, it is time to preprocess the data and prepare to feed it to our neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmCgUfftXKiW"
      },
      "source": [
        "### Reshaping\n",
        "\n",
        "In order to use convolution layers we need to reshape our data and add a color channel to it. As you've noticed currently every digit has a shape of `(100, 100)` which means that it is a 100x100 matrix of values form `0` to `1`. We need to reshape it to `(100, 100, 1)` shape so that each pixel potentially may have multiple channels (like Red, Green and Blue)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp0CyGlIw5sH"
      },
      "outputs": [],
      "source": [
        "x_train_with_channels = x_train.reshape(\n",
        "    x_train.shape[0],\n",
        "    IMAGE_WIDTH,\n",
        "    IMAGE_HEIGHT,\n",
        "    IMAGE_CHANNELS\n",
        ")\n",
        "\n",
        "x_test_with_channels = x_test.reshape(\n",
        "    x_test.shape[0],\n",
        "    IMAGE_WIDTH,\n",
        "    IMAGE_HEIGHT,\n",
        "    IMAGE_CHANNELS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOLV-oHmXZ-P"
      },
      "outputs": [],
      "source": [
        "print('x_train_with_channels:', x_train_with_channels.shape)\n",
        "print('x_test_with_channels:', x_test_with_channels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHQiYJZBXgo4"
      },
      "source": [
        "### Normalize the data\n",
        "\n",
        "Normalization gives equal weights/importance to each variable so that no single variable steers model performance in one direction just because they are bigger numbers.\n",
        "\n",
        "Here we're just trying to move from values range of `[0...255]` to `[0...1]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixKdh8hj8MW2"
      },
      "outputs": [],
      "source": [
        "x_train_normalized = x_train_with_channels / 255.0\n",
        "x_test_normalized = x_test_with_channels / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nhhy3F_Y1oB"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "We will use [Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential?version=stable) Keras model.\n",
        "\n",
        "Then we will have two pairs of [Convolution2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D?version=stable) and [MaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D?version=stable) layers. The MaxPooling layer acts as a sort of downsampling using max values in a region instead of averaging.\n",
        "\n",
        "After that we will use [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten?version=stable) layer to convert multidimensional parameters to vector.\n",
        "\n",
        "The last layer will be a [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable) layer with `34` [Softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax?version=stable) outputs. The output represents the network guess.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwxLW53rxB8P"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Input((100, 100, 1)))\n",
        "model.add(Conv2D(filters=32, kernel_size=7, activation=relu, padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(5, 5)))\n",
        "model.add(Conv2D(filters=64, kernel_size=7, activation=relu, padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(5, 5)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=256, activation=relu))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=34, activation=softmax))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB8UA21IHHnT"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6_d1Ep2OoRF"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model,\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8PvMM5iZo9W"
      },
      "source": [
        "### Model Compilation\n",
        "\n",
        "We will be using the default keras optimizer: Adam. However, you can experiment with different optimizers such as SGD or RMSprop and compare the results.\n",
        "\n",
        "As this is multi-class categorical problem, using Categorical Crossentropy would be the most optimal loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt5nXqoH4aBF"
      },
      "outputs": [],
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(opt, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1TgJtHXaSBr"
      },
      "source": [
        "### Model Training\n",
        "\n",
        "Specify the hyperparameters and start training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBctLTlBxyaL"
      },
      "outputs": [],
      "source": [
        "training_history = model.fit(\n",
        "    x_train_normalized,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        "    shuffle=True,\n",
        "    validation_data=(x_test_normalized, y_test)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF95rpUkbYM_"
      },
      "source": [
        "## Training Results\n",
        "\n",
        "Visualize training results with graphs and images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGFaV5XJ1GjE"
      },
      "outputs": [],
      "source": [
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.plot(training_history.history['accuracy'], 'b', label='Training Accuracy')\n",
        "plt.plot(training_history.history['val_accuracy'], 'r', label='Validation Accuracy')\n",
        "plt.title('Accuracy Graph')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(training_history.history['loss'], 'b', label='Training Loss')\n",
        "plt.plot(training_history.history['val_loss'], 'r', label='Validation Loss')\n",
        "plt.title('Loss Graph')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxaPQ_h5dBLf"
      },
      "source": [
        "### Model Accuracy Evaluation\n",
        "\n",
        "We need to compare the accuracy of our model on **training** set and on **test** set. We expect our model to perform similarly on both sets. If the performance on a test set will be poor comparing to a training set it would be an indicator for us that the model is overfitted and we have a \"high variance\" issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_F42B71dQhT"
      },
      "outputs": [],
      "source": [
        "train_loss, train_accuracy = model.evaluate(x_train_normalized, y_train)\n",
        "validation_loss, validation_accuracy = model.evaluate(x_test_normalized, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSy53RYDyEgy"
      },
      "outputs": [],
      "source": [
        "print(\"Training Loss:\", train_loss)\n",
        "print(\"Training Accuracy:\", train_accuracy, '\\n')\n",
        "\n",
        "print(\"Test Loss:\", validation_loss)\n",
        "print(\"Test Accuracy:\", validation_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BEoU559d8K-"
      },
      "source": [
        "## Model Predicting\n",
        "\n",
        "To use the model that we've just trained for character recognition we need to call `predict()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPhVaDjUe_VS"
      },
      "outputs": [],
      "source": [
        "predictions_one_hot = model.predict([x_test_normalized])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp0UpYlDkXn_"
      },
      "source": [
        "Each prediction consists of 34 probabilities (one for each character). We need to pick the one with the highest probability since this would be the character that our model is most confident with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbqhRfqSfALd"
      },
      "outputs": [],
      "source": [
        "# Predictions in form of one-hot vectors (arrays of probabilities).\n",
        "pd.DataFrame(predictions_one_hot)\n",
        "\n",
        "# Extract predictions with highest probabilites and detect what characters have been actually recognized.\n",
        "predictions = np.argmax(predictions_one_hot, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmYXy0igyW6a"
      },
      "outputs": [],
      "source": [
        "def plot_data(indexes):\n",
        "  num_cells = math.ceil(math.sqrt(len(indexes)))\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  for i, index in enumerate(indexes):\n",
        "    predicted_label = alphabet[predictions[index]]\n",
        "    actual_label = alphabet[y_test[index]]\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    color_map = 'Greens' if predicted_label == actual_label else 'Reds'\n",
        "    plt.subplot(num_cells, num_cells, i + 1)\n",
        "    plt.imshow(x_test_normalized[index].reshape((IMAGE_WIDTH, IMAGE_HEIGHT)), cmap=color_map)\n",
        "    plt.xlabel(f'{predicted_label} ({actual_label})')\n",
        "\n",
        "  plt.subplots_adjust(hspace=1, wspace=0.5)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O9zBQvjkdEv"
      },
      "source": [
        "Let's print some random test examples and their corresponding predictions to see how our model performs and where it does mistakes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA7tuPfsfTJW"
      },
      "outputs": [],
      "source": [
        "amount_to_display = 144\n",
        "indexes = random.sample(range(len(x_test_normalized)), amount_to_display)\n",
        "plot_data(indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpXtPiGlwLSW"
      },
      "source": [
        "Now, let's view some of the test samples for which the model had evaluated wrongly on. We see that most of these samples are quite messy and even humans may sometimes misread them, much less an AI. Thus, our AI has achieved its objective at recognizing cyrillic characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhIqmsZkuTaH"
      },
      "outputs": [],
      "source": [
        "wrong_indexes = np.where(predictions != y_test)[0]\n",
        "plot_data(wrong_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGdDpW04kkfx"
      },
      "source": [
        "## Plotting a confusion matrix\n",
        "\n",
        "[Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) shows what numbers are recognized well by the model and what numbers the model usually confuses to recognize correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI3fOqwYkmFf"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = tf.math.confusion_matrix(y_test, predictions)\n",
        "f, ax = plt.subplots(figsize=(9, 7))\n",
        "sn.heatmap(\n",
        "    confusion_matrix,\n",
        "    annot=True,\n",
        "    linewidths=.5,\n",
        "    fmt=\"d\",\n",
        "    square=True,\n",
        "    ax=ax\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running on Submission Data"
      ],
      "metadata": {
        "id": "BPIcLctS6AWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just run the model on test.csv and save the results to submission.csv"
      ],
      "metadata": {
        "id": "hpH3vfKa6L8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = \"package/test\"\n",
        "df = pd.read_csv(\"package/test.csv\")\n",
        "\n",
        "images = []\n",
        "\n",
        "for label in df['id']:\n",
        "    image_path = os.path.join(image_folder, label + '.jpg')\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    image = 1 - image / 255.0\n",
        "    images.append(image)\n",
        "\n",
        "images = np.array(images)\n",
        "images = images.reshape(images.shape[0], IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)\n",
        "images.shape"
      ],
      "metadata": {
        "id": "QjCjWjYaDnK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_one_hot = model.predict([images])\n",
        "predictions = np.argmax(predictions_one_hot, axis=1)"
      ],
      "metadata": {
        "id": "R9mue3btFHz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = [alphabet[x] for x in predictions]\n",
        "df.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "NXCp8EfFFQog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEMOmGlnai7N"
      },
      "source": [
        "## Model Saving\n",
        "\n",
        "Once you have trained your model, you might want to save your model to export it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yxG5pJIasIh"
      },
      "outputs": [],
      "source": [
        "model_name = 'cyrillic_recognition_cnn.h5'\n",
        "model.save(model_name, save_format='h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}